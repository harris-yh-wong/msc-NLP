{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Harris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Harris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.59.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.9.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.7)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (5.2.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2022.6.15)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\harris\\anaconda3\\envs\\msc-nlp\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 20:56:38.045585: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-07-12 20:56:38.046459: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeugma, Theano (for GloVe embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeugma.embeddings import EmbeddingTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "## sklearn also has some nice funtions for representations\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Classifiers\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "## and for evaluation\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn import metrics\n",
    "# from sklearn.model_selection import cross_validate\n",
    "# from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "proj_dir = Path('.')/'..'/'..'\n",
    "source_data_dir = proj_dir/'data'/'source'\n",
    "clean_data_dir  = proj_dir/'data'/'clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Process source files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 30135 lines from ..\\..\\data\\source\\pubmed\\test.txt\n",
      "Wrote 30135 lines to ..\\..\\data\\clean\\pubmed\\test_processed.txt\n",
      "Read 180040 lines from ..\\..\\data\\source\\pubmed\\train.txt\n",
      "Wrote 180040 lines to ..\\..\\data\\clean\\pubmed\\train_processed.txt\n",
      "Read 30212 lines from ..\\..\\data\\source\\pubmed\\dev.txt\n",
      "Wrote 30212 lines to ..\\..\\data\\clean\\pubmed\\dev_processed.txt\n"
     ]
    }
   ],
   "source": [
    "preprocessing.process_source_files(input_dir = source_data_dir/'pubmed', output_dir = clean_data_dir/'pubmed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw, dev_raw, test_raw = preprocessing.import_processed_files(dir=clean_data_dir/'pubmed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make experimental datasets (quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_df(df, n_pmid = 2):\n",
    "    pmid_select = df['pmid'].unique().tolist()[0:n_pmid]\n",
    "    sampled = df.query('pmid in @pmid_select')\n",
    "    return sampled\n",
    "\n",
    "TEST_RUN = True\n",
    "if TEST_RUN:\n",
    "    N_PMID = 10\n",
    "    train = sample_df(train_raw, n_pmid=N_PMID)    \n",
    "    dev   = sample_df(dev_raw, n_pmid=N_PMID)\n",
    "    test  = sample_df(test_raw, n_pmid=N_PMID)\n",
    "else:\n",
    "    train = train_raw.copy()\n",
    "    dev   = dev_raw.copy()\n",
    "    test  = test_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- format datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocessing.format_df(train)\n",
    "dev   = preprocessing.format_df(dev)\n",
    "test  = preprocessing.format_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inside_brackets': '\\\\(([^\\\\)]+)\\\\)',\n",
      " 'integers_only': '(?<![\\\\d.])[0-9]+(?![\\\\d.])'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"regexes.json\") as fp:\n",
    "    regexes = json.load(fp)\n",
    "\n",
    "pprint(regexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy_stopwords = nlp.Defaults.stop_words\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "venn2([spacy_stopwords, nltk_stopwords], ['spacy', 'nltk']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = dict()\n",
    "transformers['BoW'] = CountVectorizer(\n",
    "    tokenizer=word_tokenize,  # from nltk\n",
    "    stop_words=nltk_stopwords,\n",
    "    max_features=500,\n",
    "    ngram_range=(1,1) \n",
    ")\n",
    "transformers['tfidf'] = TfidfVectorizer(\n",
    "    tokenizer=word_tokenize, # from nltk\n",
    "    stop_words=nltk_stopwords\n",
    ")\n",
    "transformers['glove'] = EmbeddingTransformer('glove') # from zeugma\n",
    "\n",
    "representations = dict()\n",
    "representations['train'] = dict()\n",
    "representations['dev']   = dict()\n",
    "representations['test']  = dict()\n",
    "\n",
    "for repr_name, transformer in transformers.items():\n",
    "    ###  transformers within the dictionary are now fitted\n",
    "    transformer.fit(train['txt'])\n",
    "\n",
    "    for data_name, dataset in zip(('train', ), (train, )):\n",
    "        ### transform the dataset\n",
    "        representation = transformer.transform(dataset['txt'])\n",
    "        ### formatting\n",
    "        if type(representation) != np.ndarray:\n",
    "            representation = representation.toarray() \n",
    "        representation = pd.DataFrame(representation)\n",
    "        ### output\n",
    "        representations[data_name][repr_name] = representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- preview the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.095088</td>\n",
       "      <td>0.075003</td>\n",
       "      <td>-0.340403</td>\n",
       "      <td>0.067870</td>\n",
       "      <td>0.048966</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.834002</td>\n",
       "      <td>-0.535745</td>\n",
       "      <td>0.157901</td>\n",
       "      <td>0.147625</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226524</td>\n",
       "      <td>0.111622</td>\n",
       "      <td>0.153001</td>\n",
       "      <td>-0.029100</td>\n",
       "      <td>-0.495260</td>\n",
       "      <td>-0.345610</td>\n",
       "      <td>0.125865</td>\n",
       "      <td>-0.060836</td>\n",
       "      <td>-0.216941</td>\n",
       "      <td>-0.229318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.155271</td>\n",
       "      <td>0.142241</td>\n",
       "      <td>-0.092868</td>\n",
       "      <td>-0.181030</td>\n",
       "      <td>0.172117</td>\n",
       "      <td>-0.110830</td>\n",
       "      <td>0.773499</td>\n",
       "      <td>-0.448813</td>\n",
       "      <td>-0.185371</td>\n",
       "      <td>0.029634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304260</td>\n",
       "      <td>0.287251</td>\n",
       "      <td>0.170847</td>\n",
       "      <td>0.027117</td>\n",
       "      <td>-0.499730</td>\n",
       "      <td>-0.344393</td>\n",
       "      <td>0.112428</td>\n",
       "      <td>-0.247847</td>\n",
       "      <td>-0.091484</td>\n",
       "      <td>-0.314186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.151961</td>\n",
       "      <td>-0.069922</td>\n",
       "      <td>-0.689118</td>\n",
       "      <td>0.158852</td>\n",
       "      <td>0.281259</td>\n",
       "      <td>0.045158</td>\n",
       "      <td>0.527157</td>\n",
       "      <td>-1.080880</td>\n",
       "      <td>0.317614</td>\n",
       "      <td>0.012207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121966</td>\n",
       "      <td>0.099242</td>\n",
       "      <td>0.089910</td>\n",
       "      <td>0.185861</td>\n",
       "      <td>-0.392683</td>\n",
       "      <td>-0.382372</td>\n",
       "      <td>0.078356</td>\n",
       "      <td>0.068270</td>\n",
       "      <td>0.095844</td>\n",
       "      <td>-0.425218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.069870</td>\n",
       "      <td>-0.225407</td>\n",
       "      <td>-0.236780</td>\n",
       "      <td>-0.025364</td>\n",
       "      <td>0.327557</td>\n",
       "      <td>0.198099</td>\n",
       "      <td>0.656091</td>\n",
       "      <td>0.070311</td>\n",
       "      <td>0.348411</td>\n",
       "      <td>0.219523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.274355</td>\n",
       "      <td>-0.003218</td>\n",
       "      <td>0.459525</td>\n",
       "      <td>-0.168478</td>\n",
       "      <td>-0.093817</td>\n",
       "      <td>-0.344218</td>\n",
       "      <td>-0.222734</td>\n",
       "      <td>0.161007</td>\n",
       "      <td>0.128783</td>\n",
       "      <td>-0.101061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019597</td>\n",
       "      <td>-0.108223</td>\n",
       "      <td>-0.366440</td>\n",
       "      <td>0.097416</td>\n",
       "      <td>-0.029558</td>\n",
       "      <td>0.082568</td>\n",
       "      <td>0.916761</td>\n",
       "      <td>0.007552</td>\n",
       "      <td>0.225738</td>\n",
       "      <td>0.181114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.468520</td>\n",
       "      <td>-0.154558</td>\n",
       "      <td>0.110191</td>\n",
       "      <td>-0.146353</td>\n",
       "      <td>-0.305023</td>\n",
       "      <td>-0.254590</td>\n",
       "      <td>-0.142415</td>\n",
       "      <td>-0.066023</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>-0.186979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.095088  0.075003 -0.340403  0.067870  0.048966  0.005249  0.834002   \n",
       "1 -0.155271  0.142241 -0.092868 -0.181030  0.172117 -0.110830  0.773499   \n",
       "2 -0.151961 -0.069922 -0.689118  0.158852  0.281259  0.045158  0.527157   \n",
       "3  0.069870 -0.225407 -0.236780 -0.025364  0.327557  0.198099  0.656091   \n",
       "4  0.019597 -0.108223 -0.366440  0.097416 -0.029558  0.082568  0.916761   \n",
       "\n",
       "         7         8         9   ...        15        16        17        18  \\\n",
       "0 -0.535745  0.157901  0.147625  ... -0.226524  0.111622  0.153001 -0.029100   \n",
       "1 -0.448813 -0.185371  0.029634  ... -0.304260  0.287251  0.170847  0.027117   \n",
       "2 -1.080880  0.317614  0.012207  ...  0.121966  0.099242  0.089910  0.185861   \n",
       "3  0.070311  0.348411  0.219523  ... -0.274355 -0.003218  0.459525 -0.168478   \n",
       "4  0.007552  0.225738  0.181114  ... -0.468520 -0.154558  0.110191 -0.146353   \n",
       "\n",
       "         19        20        21        22        23        24  \n",
       "0 -0.495260 -0.345610  0.125865 -0.060836 -0.216941 -0.229318  \n",
       "1 -0.499730 -0.344393  0.112428 -0.247847 -0.091484 -0.314186  \n",
       "2 -0.392683 -0.382372  0.078356  0.068270  0.095844 -0.425218  \n",
       "3 -0.093817 -0.344218 -0.222734  0.161007  0.128783 -0.101061  \n",
       "4 -0.305023 -0.254590 -0.142415 -0.066023 -0.001224 -0.186979  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representations['train']['glove'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- preview features -- are there any abnormalities? uppercase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoW_feats = transformers['BoW'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24293578_0    To investigate the efficacy of 6 weeks of dail...\n",
       "24293578_1    A total of 125 patients with primary knee OA w...\n",
       "24293578_2    Outcome measures included pain reduction and i...\n",
       "24293578_3    Pain was assessed using the visual analog pain...\n",
       "24293578_4    Secondary outcome measures included the Wester...\n",
       "                                    ...                        \n",
       "25406902_3    In this study , we tested 72 FH + and 32 FH - ...\n",
       "25406902_4    FH + youths had significantly greater cerebral...\n",
       "25406902_5    Additionally , FH + youths had moderately slow...\n",
       "25406902_6    Our findings suggest that global activation in...\n",
       "25406902_7    This pattern of increased activations in FH + ...\n",
       "Name: txt, Length: 118, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['youths']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [w for w in BoW_feats if not w.islower()]\n",
    "[w for w in BoW_feats if w == 'youths']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ set up classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = dict()\n",
    "pipelines['A'] = Pipeline([\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=3)),\n",
    "])\n",
    "pipelines['B'] = Pipeline([\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ Run cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = 3\n",
    "SCORE = 'f1_micro'\n",
    "SEED = 42\n",
    "rng = np.random.RandomState(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = []\n",
    "run_names  = []\n",
    "for repr_name, representation in representations['train'].items():\n",
    "  for pipe_name, pipe in pipelines.items():\n",
    "      run_name = (repr_name, pipe_name)\n",
    "      cv_result = cross_val_score(\n",
    "        pipe,\n",
    "        X=representation,\n",
    "        y=train['label'], \n",
    "        scoring=SCORE, \n",
    "        cv=CV\n",
    "      )\n",
    "      cv_results.append(cv_result)\n",
    "      run_names.append(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>representation</th>\n",
       "      <th>pipeline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">BoW</th>\n",
       "      <th>A</th>\n",
       "      <td>0.300</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.325</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.256410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">tfidf</th>\n",
       "      <th>A</th>\n",
       "      <td>0.525</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.512821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">glove</th>\n",
       "      <th>A</th>\n",
       "      <td>0.425</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.487179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.400</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0         1         2\n",
       "representation pipeline                           \n",
       "BoW            A         0.300  0.333333  0.230769\n",
       "               B         0.325  0.384615  0.256410\n",
       "tfidf          A         0.525  0.487179  0.512821\n",
       "               B         0.625  0.435897  0.461538\n",
       "glove          A         0.425  0.307692  0.487179\n",
       "               B         0.400  0.307692  0.461538"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_df = pd.DataFrame(cv_results, index=run_names)\n",
    "cv_results_df.index = pd.MultiIndex.from_tuples(run_names, names=['representation', 'pipeline'])\n",
    "cv_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ Best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_repr_name = 'tfidf'\n",
    "best_pipe_name = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  BACKGROUND       0.21      0.36      0.27        11\n",
      " CONCLUSIONS       0.60      0.60      0.60        15\n",
      "     METHODS       0.60      0.30      0.40        40\n",
      "   OBJECTIVE       0.33      0.14      0.20         7\n",
      "     RESULTS       0.54      0.75      0.63        40\n",
      "\n",
      "    accuracy                           0.50       113\n",
      "   macro avg       0.46      0.43      0.42       113\n",
      "weighted avg       0.52      0.50      0.48       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_transformer = transformers[best_repr_name]\n",
    "best_pipe    = clone(pipelines[best_pipe_name])\n",
    "\n",
    "representations['test'][best_repr_name] = best_transformer.transform(test['txt'])\n",
    "\n",
    "best_pipe.fit(\n",
    "    X=representations['train'][best_repr_name],\n",
    "    y=train['label']\n",
    ")\n",
    "predicted = best_pipe.predict(representations['test'][best_repr_name])\n",
    "\n",
    "print(classification_report(test['label'], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- preview what a sentence looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacynlp            spacylemma          spacypos            \n",
      "------------------------------------------------------------\n",
      "A                   a                   DET                 \n",
      "total               total               NOUN                \n",
      "of                  of                  ADP                 \n",
      "125                 125                 NUM                 \n",
      "patients            patient             NOUN                \n",
      "with                with                ADP                 \n",
      "primary             primary             ADJ                 \n",
      "knee                knee                NOUN                \n",
      "OA                  OA                  PROPN               \n",
      "were                be                  AUX                 \n",
      "randomized          randomize           VERB                \n",
      "1:1                 1:1                 NUM                 \n",
      ";                   ;                   PUNCT               \n",
      "63                  63                  NUM                 \n",
      "received            receive             VERB                \n",
      "7.5                 7.5                 NUM                 \n",
      "mg                  mg                  ADJ                 \n",
      "/                   /                   SYM                 \n",
      "day                 day                 NOUN                \n",
      "of                  of                  ADP                 \n",
      "prednisolone        prednisolone        NOUN                \n",
      "and                 and                 CCONJ               \n",
      "62                  62                  NUM                 \n",
      "received            receive             VERB                \n",
      "placebo             placebo             NOUN                \n",
      "for                 for                 ADP                 \n",
      "6                   6                   NUM                 \n",
      "weeks               week                NOUN                \n",
      ".                   .                   PUNCT               \n"
     ]
    }
   ],
   "source": [
    "spacyoutput = preprocessing.apply_spacy(train2clean['clean'], parser=nlp)\n",
    "row = spacyoutput.iloc[1, ]\n",
    "preprocessing.print_row(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfVectorizer()\n",
    "tfidf_transformer_fitted = tfidf_transformer.fit(df['txt'])\n",
    "\n",
    "tfidf_mat   = tfidf_transformer_fitted.transform(df['txt']) # scipy.sparse.csr.csr_matrix\n",
    "tfidf_feats = tfidf_transformer_fitted.get_feature_names() # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['001',\n",
       " '05',\n",
       " '10',\n",
       " '100',\n",
       " '12',\n",
       " '125',\n",
       " '144',\n",
       " '15',\n",
       " '18',\n",
       " '26',\n",
       " '29',\n",
       " '34',\n",
       " '62',\n",
       " '63',\n",
       " '65',\n",
       " '6mwd',\n",
       " '85',\n",
       " '86',\n",
       " '95',\n",
       " 'account',\n",
       " 'actual',\n",
       " 'ad',\n",
       " 'adults',\n",
       " 'affective',\n",
       " 'after',\n",
       " 'aim',\n",
       " 'allocation',\n",
       " 'an',\n",
       " 'analog',\n",
       " 'and',\n",
       " 'arms',\n",
       " 'assessed',\n",
       " 'assessment',\n",
       " 'assigned',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'attention',\n",
       " 'attentional',\n",
       " 'attenuation',\n",
       " 'based',\n",
       " 'be',\n",
       " 'behavior',\n",
       " 'better',\n",
       " 'between',\n",
       " 'bias',\n",
       " 'biases',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'caloric',\n",
       " 'changes',\n",
       " 'ci',\n",
       " 'clinical',\n",
       " 'clinically',\n",
       " 'clinicaltrials',\n",
       " 'cognitive',\n",
       " 'compared',\n",
       " 'condition',\n",
       " 'conditions',\n",
       " 'contribute',\n",
       " 'contributing',\n",
       " 'cues',\n",
       " 'current',\n",
       " 'daily',\n",
       " 'day',\n",
       " 'debq',\n",
       " 'development',\n",
       " 'did',\n",
       " 'difference',\n",
       " 'differences',\n",
       " 'disguised',\n",
       " 'distance',\n",
       " 'dose',\n",
       " 'during',\n",
       " 'dutch',\n",
       " 'eating',\n",
       " 'effect',\n",
       " 'efficacy',\n",
       " 'either',\n",
       " 'elevated',\n",
       " 'emotional',\n",
       " 'empirical',\n",
       " 'equivocal',\n",
       " 'evidence',\n",
       " 'expected',\n",
       " 'experimental',\n",
       " 'experimentally',\n",
       " 'eye',\n",
       " 'factor',\n",
       " 'findings',\n",
       " 'food',\n",
       " 'foods',\n",
       " 'for',\n",
       " 'function',\n",
       " 'further',\n",
       " 'general',\n",
       " 'global',\n",
       " 'gov',\n",
       " 'grade',\n",
       " 'group',\n",
       " 'had',\n",
       " 'healthy',\n",
       " 'hierarchical',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'hscrp',\n",
       " 'identifier',\n",
       " 'if',\n",
       " 'il',\n",
       " 'improvement',\n",
       " 'improving',\n",
       " 'in',\n",
       " 'included',\n",
       " 'increased',\n",
       " 'index',\n",
       " 'individual',\n",
       " 'induced',\n",
       " 'induction',\n",
       " 'inflammation',\n",
       " 'intake',\n",
       " 'interleukin',\n",
       " 'international',\n",
       " 'intervention',\n",
       " 'investigate',\n",
       " 'is',\n",
       " 'it',\n",
       " 'knee',\n",
       " 'laboratory',\n",
       " 'least',\n",
       " 'less',\n",
       " 'levels',\n",
       " 'libitum',\n",
       " 'longer',\n",
       " 'low',\n",
       " 'maintenance',\n",
       " 'markers',\n",
       " 'maybe',\n",
       " 'mcmaster',\n",
       " 'mean',\n",
       " 'measured',\n",
       " 'measures',\n",
       " 'mechanism',\n",
       " 'mechanisms',\n",
       " 'mg',\n",
       " 'might',\n",
       " 'min',\n",
       " 'mm',\n",
       " 'mobility',\n",
       " 'modeling',\n",
       " 'moderate',\n",
       " 'moderates',\n",
       " 'mood',\n",
       " 'motivation',\n",
       " 'multivariate',\n",
       " 'nct01619163',\n",
       " 'necrosis',\n",
       " 'neutral',\n",
       " 'not',\n",
       " 'oa',\n",
       " 'obesity',\n",
       " 'of',\n",
       " 'offer',\n",
       " 'older',\n",
       " 'on',\n",
       " 'one',\n",
       " 'ontario',\n",
       " 'or',\n",
       " 'oral',\n",
       " 'osteoarthritis',\n",
       " 'outcome',\n",
       " 'overeating',\n",
       " 'overeats',\n",
       " 'pain',\n",
       " 'participants',\n",
       " 'patient',\n",
       " 'patients',\n",
       " 'pga',\n",
       " 'physical',\n",
       " 'pictorial',\n",
       " 'placebo',\n",
       " 'predict',\n",
       " 'predictive',\n",
       " 'predicts',\n",
       " 'prednisolone',\n",
       " 'primary',\n",
       " 'probe',\n",
       " 'protein',\n",
       " 'questionnaire',\n",
       " 'randomized',\n",
       " 'randomly',\n",
       " 'rate',\n",
       " 'reactive',\n",
       " 'received',\n",
       " 'reduction',\n",
       " 'regression',\n",
       " 'related',\n",
       " 'relates',\n",
       " 'relevant',\n",
       " 'remain',\n",
       " 'remained',\n",
       " 'reported',\n",
       " 'research',\n",
       " 'respectively',\n",
       " 'responder',\n",
       " 'resulting',\n",
       " 'results',\n",
       " 'rheumatology',\n",
       " 'sad',\n",
       " 'scale',\n",
       " 'scores',\n",
       " 'secondary',\n",
       " 'self',\n",
       " 'sensitivity',\n",
       " 'serum',\n",
       " 'setting',\n",
       " 'severe',\n",
       " 'severity',\n",
       " 'short',\n",
       " 'show',\n",
       " 'showed',\n",
       " 'significant',\n",
       " 'significantly',\n",
       " 'society',\n",
       " 'specifically',\n",
       " 'state',\n",
       " 'stimuli',\n",
       " 'study',\n",
       " 'suggest',\n",
       " 'sustained',\n",
       " 'systemic',\n",
       " 'task',\n",
       " 'term',\n",
       " 'test',\n",
       " 'tested',\n",
       " 'that',\n",
       " 'the',\n",
       " 'there',\n",
       " 'therefore',\n",
       " 'these',\n",
       " 'this',\n",
       " 'tnf',\n",
       " 'to',\n",
       " 'total',\n",
       " 'tracking',\n",
       " 'trait',\n",
       " 'treatment',\n",
       " 'trials',\n",
       " 'tumor',\n",
       " 'two',\n",
       " 'universities',\n",
       " 'using',\n",
       " 'validly',\n",
       " 'versus',\n",
       " 'visual',\n",
       " 'vs',\n",
       " 'walk',\n",
       " 'was',\n",
       " 'weeks',\n",
       " 'were',\n",
       " 'western',\n",
       " 'when',\n",
       " 'whether',\n",
       " 'who',\n",
       " 'with',\n",
       " 'women',\n",
       " 'would',\n",
       " 'yet']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['( Funded by the National Health and Medical Research Council ; ONTRAC Australian New Zealand Clinical Trials Registry number , ACTRN12612000625875 . )',\n",
       " 'The lowest concentration ( 0.0625 % in 50 ml ) and volume ( 0.5 ml of 1 % solution ) levels of urea solution were obtained that can exert negative influence on the results of helicobacteriosis diagnosis by means of mouth cavity air analysis .']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[test['txt'].str.contains(\"0625\"), 'txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”´ NUMBERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there is some problem with the hyphen not separated by dashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOT USED\n",
    "\n",
    "if False:\n",
    "    import string\n",
    "\n",
    "    exceptions = '.,-'\n",
    "    punctuations = [char for char in string.punctuation if char not in exceptions]\n",
    "    punctuations = ''.join(punctuations)\n",
    "    punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180040, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['To investigate the efficacy of 6 weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at 12 weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .',\n",
       " 'A total of 125 patients with primary knee OA were randomized 1:1 ; 63 received 7.5 mg/day of prednisolone and 62 received placebo for 6 weeks .',\n",
       " 'Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_index(sent):\n",
    "    flag = train['txt'].str.contains(sent)\n",
    "    return train.loc[flag].index\n",
    "\n",
    "def compare_cleaned(row):\n",
    "    for i in range(train2clean.shape[0]):\n",
    "        row = train2clean.iloc[i]\n",
    "        print(row.name)\n",
    "        print(row['txt'])\n",
    "        print(row['clean'])\n",
    "        print('\\n', end='')\n",
    "\n",
    "\n",
    "train2clean = train_raw.copy()\n",
    "train2clean['clean'] = train2clean['txt'].copy() # make a copy\n",
    "\n",
    "### subset\n",
    "## EITHER: find by regex\n",
    "# indices = find_index(\"\")\n",
    "# train2clean = train2clean.loc[indices, ]\n",
    "## OR:     locate by indices\n",
    "# train2clean = train2clean.iloc[1:2, ]\n",
    "## OR:     everything\n",
    "train2clean = train2clean\n",
    "\n",
    "print(train2clean.shape)\n",
    "train2clean.head(3)['txt'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- apply replacements (regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected_hyphen (?<=[0-9])-(?=[0-9])  -  180040\n",
      "truncated_floats (?<=[0-9]) \\.(?=[0-9]) . 180040\n",
      "num_comma_separator (?<=[0-9]) ,(?=[0-9])  180040\n",
      "flag_integers (?<![\\d.])[0-9]+(?![\\d.]) @INTEGER 180040\n",
      "flag_floats [+-]?(?=\\d*[.eE])([0-9]+\\.?[0-9]*|\\.[0-9]+)([eE][+-]?[0-9]+)? @FLOAT 180040\n"
     ]
    }
   ],
   "source": [
    "replacements = dict()\n",
    "replacements['connected_hyphen'] = {\n",
    "    'regex': f\"(?<=[0-9])-(?=[0-9])\",\n",
    "    'replace_to': \" - \"\n",
    "}\n",
    "replacements['truncated_floats'] = {\n",
    "    'regex': \"(?<=[0-9]) \\.(?=[0-9])\",\n",
    "    'replace_to': \".\"\n",
    "}\n",
    "replacements['num_comma_separator'] = {\n",
    "    'regex': \"(?<=[0-9]) ,(?=[0-9])\",\n",
    "    'replace_to': \"\"\n",
    "}\n",
    "replacements['flag_integers'] = {\n",
    "    'regex': \"(?<![\\\\d.])[0-9]+(?![\\\\d.])\",\n",
    "    'replace_to': \"@INTEGER\"\n",
    "}\n",
    "replacements['flag_floats'] = {\n",
    "    'regex': \"[+-]?(?=\\\\d*[.eE])([0-9]+\\\\.?[0-9]*|\\\\.[0-9]+)([eE][+-]?[0-9]+)?\",\n",
    "    'replace_to': \"@FLOAT\"\n",
    "}\n",
    "\n",
    "steps = [\n",
    "    'connected_hyphen', \n",
    "    'truncated_floats',\n",
    "    'num_comma_separator',\n",
    "    'flag_integers',\n",
    "    'flag_floats'\n",
    "]\n",
    "flags = []\n",
    "\n",
    "for step in steps:\n",
    "    replacement = replacements[step]\n",
    "    regex = replacement['regex']\n",
    "    replace_to = replacement['replace_to']\n",
    "    \n",
    "    flag = train2clean['clean'].str.contains(regex)\n",
    "    print(step, regex, replace_to, len(flag))\n",
    "    flags.append(flag)\n",
    "    train2clean['clean'] = train2clean['clean'].str.replace(regex, replace_to)\n",
    "\n",
    "\n",
    "compare_cleaned(train2clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6', '6'] \n",
      " There was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and 6MWD at 6 weeks .\n",
      "\n",
      "['95', '18', '15', '26', '144'] \n",
      " The mean difference between treatment arms ( 95 % CI ) was 10.9 ( 4.8-18 .0 ) , p < 0.001 ; 9.5 ( 3.7-15 .4 ) , p < 0.05 ; 15.7 ( 5.3-26 .1 ) , p < 0.001 ; and 86.9 ( 29.8-144 .1 ) , p < 0.05 , respectively .\n",
      "\n",
      "['1', '6', '6'] \n",
      " Further , there was a clinically relevant reduction in the serum levels of IL-1 , IL-6 , TNF - , and hsCRP at 6 weeks in the intervention group when compared to the placebo group .\n",
      "\n",
      "['12'] \n",
      " These differences remained significant at 12 weeks .\n",
      "\n",
      "['65', '34'] \n",
      " The Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was 65 % in the intervention group and 34 % in the placebo group ( p < 0.05 ) .\n",
      "\n",
      "[] \n",
      " Hierarchical multivariate regression modeling showed that self-reported emotional eating did not account for changes in attention allocation for food or food intake in either condition .\n",
      "\n",
      "[] \n",
      " Yet , attention maintenance on food cues was significantly related to increased intake specifically in the neutral condition , but not in the sad mood condition .\n",
      "\n",
      "['95', '95'] \n",
      " Homes in the Full Education and Practice arms were more likely to have a functioning smoke alarm at follow-up ( OR = 2.77 , 95 % CI 1.09 to 7.03 ) and had an average of 0.32 more working alarms per home ( 95 % CI 0.09 to 0.56 ) .\n",
      "\n",
      "['16'] \n",
      " Working alarms per home rose 16 % .\n",
      "\n",
      "[] \n",
      " Full Education and Practice had similar effectiveness ( p = 0.97 on both outcome measures ) .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"regexes.json\") as fp:\n",
    "    regexes = json.load(fp)\n",
    "    \n",
    "regex = regexes['integers_only']\n",
    "finds = exp['txt'].str.findall(regex)\n",
    "\n",
    "for i in range(exp.shape[0]):\n",
    "    txt = exp.iloc[i]['txt']\n",
    "    find = finds.iloc[i]\n",
    "    print(find, '\\n', txt, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”´ Tense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- any "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_counts = (\n",
    "    df[['label', 'spacylemma']]\n",
    "    .explode('spacylemma')\n",
    ")\n",
    "lemma_crosstab = pd.crosstab(index=lemma_counts['spacylemma'], columns=lemma_counts['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "BACKGROUND      590\n",
      "CONCLUSIONS     305\n",
      "METHODS        4068\n",
      "OBJECTIVE       553\n",
      "RESULTS         245\n",
      "Name: randomized, dtype: int64\n",
      "label\n",
      "BACKGROUND     151\n",
      "CONCLUSIONS     46\n",
      "METHODS        744\n",
      "OBJECTIVE       64\n",
      "RESULTS         50\n",
      "Name: randomised, dtype: int64\n",
      "label\n",
      "BACKGROUND      13\n",
      "CONCLUSIONS     29\n",
      "METHODS        523\n",
      "OBJECTIVE       15\n",
      "RESULTS        217\n",
      "Name: randomization, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for token in ['randomized', 'randomised', 'randomization']:\n",
    "    print(lemma_crosstab.loc[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diskcount discount disk disc randomisation randomization randomised\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Diskcount discount disc disc randomisation randomization randomised'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "def britishize(string, us2uk_dict=None):\n",
    "    if us2uk_dict is None:\n",
    "        url =\"https://raw.githubusercontent.com/hyperreality/American-British-English-Translator/master/data/american_spellings.json\"\n",
    "        us2uk_dict = requests.get(url).json()    \n",
    "\n",
    "    for us, uk in us2uk_dict.items():\n",
    "        string = string.replace(us, uk) \n",
    "    return string\n",
    "\n",
    "url =\"https://raw.githubusercontent.com/hyperreality/American-British-English-Translator/master/data/american_spellings.json\"\n",
    "us2uk_dict = requests.get(url).json()\n",
    "\n",
    "\n",
    "\n",
    "example_sent = u'Diskcount discount disk disc randomisation randomization randomised'\n",
    "print(example_sent)\n",
    "britishize(example_sent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['randomize', 'randomized', 'randomizes', 'randomizing']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in us2uk_dict.keys() if 'random' in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomisation  ===> randomisation\n",
      "randomized  ===> randomize\n",
      "compute  ===> compute\n",
      "computed  ===> compute\n"
     ]
    }
   ],
   "source": [
    "example_sent = nlp(u'Randomisation randomized compute computed')\n",
    "\n",
    "for word in example_sent:\n",
    "    print(word.text + '  ===>', word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "tokens = ['compute', 'computer', 'computed', 'computing']\n",
    "\n",
    "for token in tokens:\n",
    "    print(token + ' --> ' + stemmer.stem(token))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('msc-NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ca4a5427f6f904b0fb59ebe4723ed6fa2277abd3f62b239565a58ff4c14b3b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
